{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOisi6xkkL/Gf+zYl8WTX3G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ednavivianasegura/ERAP_CursoPython/blob/main/Modulo3_RedesNeuronales/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Autores:** Edna Viviana Segura Alvarado - Hans Mauricio Carrillo Hernández\n",
        "\n",
        "**Institución:** Universidad de la Rioja\n",
        "\n",
        "**Fecha:** Junio/2025\n",
        "\n",
        "**Redes neuronales recurrentes (RNN)**\n",
        "\n",
        "\n",
        "Nota: basado en [RNN_C](https://www.datacamp.com/es/tutorial/tutorial-for-recurrent-neural-networks)\n"
      ],
      "metadata": {
        "id": "df4SwpVU5rnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Redes Neuronales Recurrentes (RNN)  \n",
        "\n",
        "Las **Redes Neuronales Recurrentes (RNN)** son un tipo de red neuronal artificial diseñada para trabajar con datos secuenciales (como texto, series temporales o audio). A diferencia de las redes neuronales tradicionales, las RNN mantienen una **memoria interna** que les permite recordar información de pasos anteriores, lo que las hace ideales para:  \n",
        "\n",
        "- Predicción de valores financieros.  \n",
        "- Generación de texto (traducción, resúmenes).  \n",
        "- Procesamiento de voz (ej.: Siri, Google Voice Search).  \n",
        "\n",
        "#### Características clave:  \n",
        "1. **Dependencia secuencial**: la salida en el paso `t` depende de las entradas anteriores (`t-1`, `t-2`, ...).  \n",
        "2. **Compartición de parámetros**: usan los mismos pesos en todas las capas (eficiencia computacional).  \n",
        "3. **Ajuste durante el entrenamiento**: los pesos y sesgos se optimizan mediante backpropagation a través del tiempo (BPTT).  \n",
        "\n",
        "#### vs. Redes Feed-Forward:  \n",
        "| **RNN** | **Feed-Forward** |  \n",
        "|---------|------------------|  \n",
        "| Memoria interna (estado oculto) | Sin memoria |  \n",
        "| Salidas dependientes | Salidas independientes |  \n",
        "| Ideal para secuencias | Ideal para datos estáticos |  \n"
      ],
      "metadata": {
        "id": "VqzEQFURg1Rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/AccesoImages/blob/main/RNN/1.png?raw=true\" alt=\"1\" width=\"50%\" height=\"50%\">  \n",
        "</center>"
      ],
      "metadata": {
        "id": "pjwAVH53hgp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funcionamiento de las Redes Neuronales Recurrentes (RNN)\n",
        "\n",
        "\n",
        "Las RNN procesan información secuencial mediante bucles recurrentes, donde cada salida depende tanto de la entrada actual como de las anteriores. Este mecanismo les permite \"recordar\" patrones temporales, algo imposible en redes neuronales tradicionales.\n",
        "\n",
        "**Arquitectura clave:**\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/AccesoImages/blob/main/RNN/2.png?raw=true\" alt=\"2\" width=\"20%\" height=\"5%\">  \n",
        "</center>\n",
        "\n",
        "* Capa de entrada (X): recibe los datos (ej.: palabras, valores temporales).\n",
        "\n",
        "* Capa oculta (A):\n",
        "\n",
        "comparte los mismos parámetros (pesos y sesgos) en todos los pasos de tiempo.\n",
        "\n",
        "Usa funciones de activación (como tanh o ReLU) para transformar los datos.\n",
        "\n",
        "* Bucle recurrente: el estado oculto se retroalimenta, combinando información nueva con la pasada.\n",
        "\n",
        "### Entrenamiento: Backpropagation Through Time (BPTT)\n",
        "\n",
        "Diferencias con backpropagation tradicional:\n",
        "\n",
        "* BPTT calcula gradientes a lo largo de la secuencia temporal, sumando errores en cada paso.\n",
        "\n",
        "* Optimiza parámetros compartidos, lo que reduce la complejidad computacional."
      ],
      "metadata": {
        "id": "Z3DXa4-GmmJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tipos de RNN y sus aplicaciones\n",
        "\n",
        "Gracias a su flexibilidad en longitudes de entrada/salida, las RNN destacan en tareas secuenciales:\n",
        "\n",
        "## Tabla Comparativa: Tipos de RNN y sus Aplicaciones\n",
        "\n",
        "| Tipo de RNN        | Estructura          | Ejemplo de Aplicación      | Caso de Uso Concreto               | Framework Implementación |\n",
        "|--------------------|---------------------|---------------------------|------------------------------------|--------------------------|\n",
        "| **Uno-a-Uno**      | `x → h → y`         | Clasificación de imágenes | MNIST/CIFAR-10                     | `torch.nn.RNN`           |\n",
        "| **Uno-a-Muchos**   | `x → h → [y₁...yₙ]` | Generación de música      | Composición de melodías MIDI       | `tf.keras.SimpleRNN`     |\n",
        "| **Muchos-a-Uno**   | `[x₁...xₙ] → h → y` | Análisis de sentimiento   | Clasificación de reseñas en Amazon | `nn.RNN` (PyTorch)       |\n",
        "| **Muchos-a-Muchos**| `[x₁...xₙ] → h → [y₁...yₙ]` | Traducción automática | Seq2Seq (Inglés-Español)          | `tf.keras.LSTM`          |"
      ],
      "metadata": {
        "id": "_Bf-GNifoN0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/AccesoImages/blob/main/RNN/3.png?raw=true\" alt=\"3\" width=\"50%\" height=\"50%\">  \n",
        "</center>"
      ],
      "metadata": {
        "id": "dXw60Jqpog-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparativa: Redes Neuronales Convolucionales (CNN) vs. Recurrentes (RNN)\n",
        "\n",
        "**Redes Neuronales Convolucionales (CNN)**\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/AccesoImages/blob/main/RNN/4.png?raw=true\" alt=\"4\" width=\"50%\" height=\"50%\">  \n",
        "</center>\n",
        "\n",
        "**Arquitectura típica:**\n",
        "\n",
        "\n",
        "```\n",
        "Conv2D → ReLU → MaxPooling → Flatten → Dense\n",
        "```\n",
        "\n",
        "**Aplicaciones principales:**\n",
        "\n",
        "\n",
        "* Clasificación de imágenes (ej: MNIST, ImageNet)\n",
        "\n",
        "* Procesamiento de video\n",
        "\n",
        "* Detección de objetos\n",
        "\n",
        "**Ventajas clave:**\n",
        "\n",
        "* Captura patrones espaciales mediante filtros convolucionales\n",
        "\n",
        "* Invariante a traslaciones en imágenes\n",
        "\n",
        "* Eficiente para datos grid-like (píxeles)\n",
        "\n",
        "**Redes Neuronales Recurrentes (RNN)**\n",
        "\n",
        "**Arquitectura típica:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "RNNCell → Tanh → Dense\n",
        "```\n",
        "\n",
        "**Aplicaciones principales:**\n",
        "\n",
        "* Procesamiento de lenguaje natural\n",
        "\n",
        "* Análisis de series temporales\n",
        "\n",
        "* Traducción automática\n",
        "\n",
        "\n",
        "### Comparativa CNN vs RNN (Formato Notebook)\n",
        "\n",
        "\n",
        "\n",
        "| Característica          | CNN                          | RNN                          |\n",
        "|-------------------------|------------------------------|------------------------------|\n",
        "| **Tipo de datos**       | Datos espaciales (imágenes)  | Datos secuenciales (texto/series) |\n",
        "| **Arquitectura**        | Capas convolucionales        | Bucles recurrentes           |\n",
        "| **Mecanismo**          | Filtros para patrones locales| Memoria de estados anteriores|\n",
        "| **Backpropagation**    | Backprop estándar            | BPTT (Through Time)          |\n",
        "| **Longitud E/S**       | Tamaño fijo                  | Longitud variable            |\n",
        "| **Ejemplo típico**     | ResNet (clasificación)       | LSTM (traducción)            |\n",
        "| **Ventaja clave**      | Invariante a traslaciones    | Modela dependencias temporales|\n",
        "| **Limitación**         | Requiere mucho dato          | Problemas de gradiente       |\n",
        "\n",
        "\n",
        "### Arquitecturas RNN Avanzadas\n",
        "LSTM (Long Short-Term Memory)\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/AccesoImages/blob/main/RNN/5.png?raw=true\" alt=\"5\" width=\"50%\" height=\"50%\">  \n",
        "</center>\n",
        "\n",
        "**Ventajas:**\n",
        "\n",
        "* Memoria a largo plazo\n",
        "\n",
        "* Evita problemas de gradiente\n",
        "\n",
        "* GRU (Gated Recurrent Unit)\n",
        "\n",
        "**Diferencias clave vs LSTM:**\n",
        "\n",
        "* Menos parámetros\n",
        "\n",
        "* Combina forget/input gates\n",
        "\n",
        "* Similar rendimiento en muchos casos\n",
        "\n"
      ],
      "metadata": {
        "id": "cvio26HOqTm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XqC1n1dfsu02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvevlZR6fqos"
      },
      "outputs": [],
      "source": []
    }
  ]
}