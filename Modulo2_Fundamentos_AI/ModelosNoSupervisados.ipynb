{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ednavivianasegura/ERAP_CursoPython/blob/main/Modulo2_Fundamentos_AI/ModelosNoSupervisados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBJp8ZNl81kJ"
      },
      "source": [
        "**Autores:** Edna Viviana Segura Alvarado - Hans Mauricio Carrillo Hernández\n",
        "\n",
        "**Institución:** Universidad de la Rioja\n",
        "\n",
        "**Fecha:** Junio/2025\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MdnAfH2-QWS"
      },
      "outputs": [],
      "source": [
        "#!pip install scikit-learn\n",
        "#!pip install matplotlib\n",
        "#!pip install pandas\n",
        "#!pip install pyclustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51mFMizs_oAq"
      },
      "outputs": [],
      "source": [
        "# @title Librerías necesarias\n",
        "\n",
        "\n",
        "# Algoritmos de clustering:\n",
        "from sklearn.cluster import KMeans  # K-means tradicional\n",
        "from sklearn.cluster import BisectingKMeans  # Versión divisiva de K-means\n",
        "\n",
        "# Visualización de datos:\n",
        "import matplotlib.pyplot as plt  # Biblioteca principal para gráficos\n",
        "\n",
        "# Cómputo numérico y manejo de arrays:\n",
        "import numpy as np  # Operaciones matemáticas eficientes\n",
        "\n",
        "# Manejo de datos estructurados (tablas):\n",
        "import pandas as pd  # Para DataFrames y análisis exploratorio\n",
        "\n",
        "# Preprocesamiento de datos:\n",
        "from sklearn.preprocessing import StandardScaler  # Estandarización (media=0, varianza=1)\n",
        "\n",
        "# Clustering jerárquico:\n",
        "from sklearn.cluster import AgglomerativeClustering  # Versión aglomerativa\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage  # Para dendrogramas y matriz de linkage\n",
        "\n",
        "# Métricas de evaluación:\n",
        "from sklearn.metrics import silhouette_score  # Calidad de clusters\n",
        "\n",
        "# Reducción de dimensionalidad:\n",
        "from sklearn.decomposition import PCA  # Análisis de Componentes Principales\n",
        "# Importación de funciones específicas para cálculo de autovalores/autovectores\n",
        "from numpy.linalg import eig  # Para descomposición espectral (usado en PCA)\n",
        "\n",
        "# Utilidades adicionales:\n",
        "from itertools import count  # Generador de contadores (útil para iteraciones)\n",
        "from sklearn.metrics import confusion_matrix, classification_report  # Para matrices de confusión (clasificación)y el cálculo de las métricas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OETz0FD09rgs"
      },
      "source": [
        "# CURSO - PYTHON: PROGRAMACIóN DE IA\n",
        "\n",
        "**Módulo: Aprendizaje no supervisado**\n",
        "\n",
        "En el aprendizaje supervisado, el sistema aprende a partir de ejemplos etiquetados, es decir, datos donde ya conocemos la respuesta correcta. El objetivo es construir un modelo que prediga etiquetas para nuevos datos. Por ejemplo: clasificar correos como \"spam\" o \"no spam\" usando ejemplos previamente etiquetados.\n",
        "\n",
        "En cambio, en el *aprendizaje no supervisado*, el sistema trabaja con datos sin etiquetas y busca patrones ocultos por sí mismo. Aquí no hay respuestas correctas de antemano; el algoritmo explora la estructura intrínseca de los datos. Un ejemplo típico es agrupar clientes con comportamientos similares sin conocer previamente las categorías.\n",
        "\n",
        "<center>\n",
        " <img src=\"https://raw.githubusercontent.com/ednavivianasegura/AccesoImages/aaa31fdeb3eb3a27d54280be8b9e45614b9a634a/clustering_diferencias.png\" alt=\"descriptiva\" width=\"50%\" height=\"50%\">  \n",
        "</center>\n",
        "\n",
        "Imagen tomada de  [Unsupervised Learning with Python: A Beginner's Guide, Vihar Kurama](https://builtin.com/data-science/unsupervised-learning-python)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Objetivos:**\n",
        "\n",
        "- Encontrar estructuras ocultas en los datos.\n",
        "\n",
        "- Agrupar datos similares (clustering).\n",
        "\n",
        "- Reducir la dimensionalidad para visualización o eficiencia.\n",
        "\n",
        "**Aplicaciones:** Segmentación de clientes, detección de anomalías, compresión de imágenes, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FACtTM3E-DYf"
      },
      "source": [
        "## Clustering\n",
        "\n",
        "El clustering (o agrupamiento) es una técnica de aprendizaje no supervisado cuyo objetivo es dividir un conjunto de datos en grupos homogéneos (llamados clusters), de tal manera que se cumpla que:\n",
        "\n",
        "* Los puntos dentro de un mismo cluster sean similares entre sí (minimizar varianza interna) *Intra-cluster*.\n",
        "\n",
        "* Los puntos de clusters distintos sean diferentes (maximizar distancia entre clusters) *Inter-cluster*.\n",
        "\n",
        "**Existen dos tipos de Clustering diferenciados por su estructura y forma de asignar clusters:**\n",
        "\n",
        "* Jerárquico\n",
        "* No jerárquico\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfuA95cskLVh"
      },
      "source": [
        "### Clustering Jerárquico\n",
        "\n",
        "Construye una jerarquía de clusters representada como un árbol (dendrograma), donde cada nodo es una fusión o división de clusters.\n",
        "\n",
        "**Subtipos:**\n",
        "\n",
        "* Aglomerativo (bottom-up): cada punto inicia como un cluster y se fusionan iterativamente.\n",
        "\n",
        "* Divisivo (top-down): todos los puntos inician en un cluster y se dividen recursivamente.\n",
        "\n",
        "**Ventaja:** No requiere especificar el número de clusters a priori.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4que1BTgQYvK"
      },
      "source": [
        "**Ejemplo:**\n",
        "\n",
        "Imaginemos que somos nutricionistas investigando la relación entre peso y estatura en un grupo de pacientes. Contamos con datos de 200 personas (peso en kg y estatura en cm), pero no tenemos información previa sobre categorías como *bajo peso*, *peso normal* o *sobrepeso*. Nuestro objetivo es descubrir si existen grupos naturales en estos datos que nos permitan identificar patrones relevantes para personalizar planes alimenticios.\n",
        "\n",
        "\n",
        "Supongamos que los datos provienen de chequeos médicos rutinarios en una clínica de nutrición durante el último año.\n",
        "\n",
        "Cada registro contiene:\n",
        "\n",
        "* Peso (kg): Variable numérica continua\n",
        "\n",
        "* Estatura (cm): Variable numérica continua\n",
        "\n",
        "Actualmente, la clínica clasifica manualmente a los pacientes en tres categorías de peso según tablas de IMC (que relacionan peso y estatura), pero sospechamos que podrían existir subgrupos más específicos que requieran intervenciones diferenciadas.\n",
        "\n",
        "* Utilizaremos clustering jerárquico y no jerárquico para identificar grupos naturales sin imponer categorías preconcebidas\n",
        "\n",
        "* Visualizar cómo se relacionan las observaciones\n",
        "\n",
        "* Determinar si las agrupaciones coinciden con las categorías tradicionales de IMC\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzPkEFnkePI2"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# 1. GENERACIÓN Y PREPARACIÓN DE DATOS\n",
        "# =============================================\n",
        "\n",
        "print(\"\\n---------------------------------------------\")\n",
        "print('GENERACIÓN Y PREPARACIÓN DE DATOS')\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "# Semilla para reproducibilidad\n",
        "np.random.seed(42)\n",
        "\n",
        "# Grupo 1: personas más bajas y livianas\n",
        "pesos_g1 = np.random.normal(loc=55, scale=6, size=80)     # Media 55 kg y desviación 6\n",
        "estaturas_g1 = np.random.normal(loc=155, scale=6, size=80)  # Media 155 cm y desviación 6\n",
        "grupo_g1 = np.full(80, 0)\n",
        "\n",
        "# Grupo 2: personas de talla media\n",
        "pesos_g2 = np.random.normal(loc=70, scale=5, size=70)     # Media 70 kg y desviación 5\n",
        "estaturas_g2 = np.random.normal(loc=165, scale=5, size=70)  # Media 165 cm y desviación 5\n",
        "grupo_g2 = np.full(70, 1)\n",
        "\n",
        "# Grupo 3: personas más altas y pesadas\n",
        "pesos_g3 = np.random.normal(loc=85, scale=3, size=50)     # Media 85 kg y desviación 3\n",
        "estaturas_g3 = np.random.normal(loc=180, scale=3, size=50)  # Media 180 cm y desviación 3\n",
        "grupo_g3 = np.full(50, 2)\n",
        "\n",
        "# Concatenar todos los datos\n",
        "pesos = np.concatenate([pesos_g1, pesos_g2, pesos_g3])\n",
        "estaturas = np.concatenate([estaturas_g1, estaturas_g2, estaturas_g3])\n",
        "grupos = np.concatenate([grupo_g1, grupo_g2, grupo_g3])\n",
        "\n",
        "# Crear DataFrame Inicial\n",
        "df_inicial = pd.DataFrame({\n",
        "    'Peso': pesos,\n",
        "    'Estatura': estaturas,\n",
        "    'Grupo': grupos\n",
        "})\n",
        "\n",
        "# Combinar las dos variables en una matriz de 200x2\n",
        "X = np.column_stack((pesos, estaturas))  # Cada fila es un paciente, columnas: [peso, estatura]\n",
        "# np.column_stack (NumPy)tiene como propósito combinar arrays 1D o 2D de NumPy en una sola matriz 2D,\n",
        "# apilándolos por columnas. Útil para unir variables numéricas antes de aplicar algoritmos de ML.\n",
        "# Útil para crear una matriz numérica para algoritmos usando SCikit-learn\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Escalado de datos (normalización)\n",
        "scaler = StandardScaler()  # Crea un objeto para estandarizar (media=0, desviación=1)\n",
        "X_scaled = scaler.fit_transform(X)  # Aplica la transformación a los datos\n",
        "\n",
        "\n",
        "print(df_inicial.groupby('Grupo').agg({\n",
        "    'Peso': ['mean', 'std'],      # Media y desviación estándar del peso\n",
        "    'Estatura': ['mean', 'std', 'count']   # Media, desviación estándar de la estatura y conteo\n",
        "}))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# 2. VISUALIZACIÓN DE DATOS\n",
        "# =============================================\n",
        "\n",
        "print(\"\\n-------------------------------------------------------------\")\n",
        "print('2. COMPARACIÓN: DATOS ORIGINALES VS ESCALADOS (Visualización)')\n",
        "print(\"---------------------------------------------------------------\")\n",
        "\n",
        "# Crear figura con 2 subplots (1 fila, 2 columnas)\n",
        "plt.figure(figsize=(15, 6))  # Ajustar tamaño para dos gráficos\n",
        "\n",
        "# ------------------------------------------\n",
        "# Gráfico 1: Datos originales\n",
        "# ------------------------------------------\n",
        "plt.subplot(1, 2, 1)  # 1 fila, 2 columnas, posición 1\n",
        "plt.scatter(\n",
        "    X[:, 0],  # Eje X: pesos originales\n",
        "    X[:, 1],  # Eje Y: estaturas originales\n",
        "    color='blue',\n",
        "    alpha=0.6\n",
        ")\n",
        "plt.xlabel('Peso (kg)')\n",
        "plt.ylabel('Estatura (cm)')\n",
        "plt.title('Datos Originales')\n",
        "plt.grid(True, alpha=0.2)\n",
        "\n",
        "# Añadir líneas de media original\n",
        "plt.axvline(x=np.mean(X[:, 0]), color='red', linestyle='--', label='Media peso')\n",
        "plt.axhline(y=np.mean(X[:, 1]), color='green', linestyle='--', label='Media estatura')\n",
        "plt.legend()\n",
        "\n",
        "# ------------------------------------------\n",
        "# Gráfico 2: Datos escalados\n",
        "# ------------------------------------------\n",
        "plt.subplot(1, 2, 2)  # 1 fila, 2 columnas, posición 2\n",
        "plt.scatter(\n",
        "    X_scaled[:, 0],  # Eje X: pesos escalados\n",
        "    X_scaled[:, 1],  # Eje Y: estaturas escaladas\n",
        "    color='purple',\n",
        "    alpha=0.6\n",
        ")\n",
        "plt.xlabel('Peso (Estandarizado)')\n",
        "plt.ylabel('Estatura (Estandarizada)')\n",
        "plt.title('Datos Estandarizados (Media=0, Desv=1)')\n",
        "plt.grid(True, alpha=0.2)\n",
        "\n",
        "# Añadir líneas en cero (medias después de escalar)\n",
        "plt.axvline(x=0, color='red', linestyle='--', label='Media peso')\n",
        "plt.axhline(y=0, color='green', linestyle='--', label='Media estatura')\n",
        "plt.legend()\n",
        "\n",
        "# Ajustar espacio entre subplots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jot2xh_2yFo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# 3. CLUSTERING JERÁRQUICO (AGGLOMERATIVE) INICIAL\n",
        "# ==================================================\n",
        "\n",
        "print(\"\\n---------------------------------------------\")\n",
        "print('CLUSTERING JERÁRQUICO (AGGLOMERATIVE) INICIAL')\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "\n",
        "# Configuración del modelo\n",
        "cluster_jerarquico = AgglomerativeClustering(\n",
        "    #n_clusters= ,       # Por defecto 2 clusters\n",
        "    metric='euclidean',  # Métrica de distancia entre puntos (distancia euclidiana)\n",
        "    linkage='ward'       # Criterio para fusionar clusters:\n",
        "    # linkage='ward': Minimiza la varianza de los clusters que se fusionan, produciendo clusters más compactos.\n",
        "    # Alternativas comunes:\n",
        "    #'complete': Usa la distancia máxima entre puntos.\n",
        "    #'average': Usa la distancia promedio.\n",
        ")\n",
        "\n",
        "# Aplicar el clustering a los datos escalados\n",
        "etiquetas_jer = cluster_jerarquico.fit_predict(X_scaled)  # Retorna un array con las etiquetas según el número\n",
        "# de clusters creados (0 o 1)\n",
        "print(etiquetas_jer)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D3VWs8j6vHxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# 4. VISUALIZACIÓN DE RESULTADOS\n",
        "# =============================================\n",
        "\n",
        "print(\"\\n ---------------------------------------------\")\n",
        "print('VISUALIZACIÓN DE RESULTADOS POR CLUSTER')\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Graficar puntos coloreados por cluster\n",
        "plt.scatter(\n",
        "    X[:, 0],           # Peso en kg (eje X)\n",
        "    X[:, 1],           # Estatura en cm (eje Y)\n",
        "    c=etiquetas_jer,       # Color según etiqueta de cluster\n",
        "    cmap='viridis',    # Mapa de colores\n",
        "    alpha=0.7          # Transparencia para mejor visualización\n",
        ")\n",
        "plt.xlabel('Peso (kg)')\n",
        "plt.ylabel('Estatura (cm)')\n",
        "plt.colorbar(label='Cluster')  # Barra lateral que indica el mapeo de colores\n",
        "plt.grid(True, alpha=0.2)\n",
        "\n",
        "# Calcular y graficar centroides\n",
        "for cluster_id in np.unique(etiquetas_jer):\n",
        "    centroide = X[etiquetas_jer == cluster_id].mean(axis=0)  # Calcula la media de cada variable por cluster\n",
        "    plt.scatter(\n",
        "        centroide[0],          # Peso promedio del cluster\n",
        "        centroide[1],          # Estatura promedio del cluster\n",
        "        marker='*',            # Forma de estrella para el centroide\n",
        "        s=200,                 # Tamaño del marcador\n",
        "        c='red',               # Color rojo\n",
        "        edgecolor='black'      # Borde negro para mejor contraste\n",
        "    )\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "rpTBVXOuvLYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# 5. ANÁLISIS ESTADÍSTICO POR CLUSTER INICIAL\n",
        "# =============================================\n",
        "\n",
        "print(\"\\n\")\n",
        "print('ANÁLISIS ESTADÍSTICO POR CLUSTER INICIAL (2)')\n",
        "print()\n",
        "\n",
        "# Convertir a DataFrame para análisis\n",
        "df = df_inicial.copy()\n",
        "df['Cluster'] = np.round(etiquetas_jer,0) # Añadir columna de clusters\n",
        "\n",
        "print(df)\n",
        "\n",
        "# Estadísticas descriptivas por cluster\n",
        "tabla= df.groupby('Cluster').agg({\n",
        "    'Peso': ['mean', 'std'],      # Media y desviación estándar del peso\n",
        "    'Estatura': ['mean', 'std', 'count']   # Media, desviación estándar de la estatura y conteo\n",
        "})\n",
        "# Total de observaciones\n",
        "total = len(df)\n",
        "\n",
        "# Extraer el conteo desde la tabla (nivel de columna múltiple)\n",
        "conteo = tabla[('Estatura', 'count')]\n",
        "\n",
        "# Calcular proporción\n",
        "proporcion = ((conteo / total).round(3))*100  # Redondear a 4 decimales\n",
        "\n",
        "# Añadir como nueva columna\n",
        "tabla[('Estatura', 'proporcion')] = proporcion\n",
        "\n",
        "print(tabla)\n",
        "\n"
      ],
      "metadata": {
        "id": "xrhvAKZrvP-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================\n",
        "# 6. DENDROGRAMA (SELECCIÓN DE NÚMERO DE CLUSTERS)\n",
        "# =================================================\n",
        "\n",
        "print(\"\\n-------------------------------------------------\")\n",
        "print('DENDROGRAMA (SELECCIÓN DE NÚMERO DE CLUSTERS)')\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "# Calcular la matriz de linkage (pasos de fusión)\n",
        "Z = linkage(\n",
        "    X_scaled,          # Datos escalados\n",
        "    method='ward'      # Mismo método que en el clustering\n",
        ")\n",
        "\n",
        "# Configurar el dendrograma\n",
        "plt.figure(figsize=(12, 5))\n",
        "dendrogram(\n",
        "    Z,                          # Matriz de linkage\n",
        "    truncate_mode='lastp',      # Mostrar solo los últimos p clusters fusionados\n",
        "    p=30,                      # Número de clusters mostrados\n",
        "    show_leaf_counts=True       # Mostrar conteo de hojas en cada rama\n",
        ")\n",
        "# Línea horizontal para sugerir número de clusters\n",
        "plt.axhline(\n",
        "    y=6,                     # Altura donde cortar el dendrograma\n",
        "    color='r',                 # Color rojo\n",
        "    linestyle='--'             # Línea discontinua\n",
        ")\n",
        "plt.title('Dendrograma - Selección de Número de Clusters')\n",
        "plt.xlabel('Índice del Paciente')\n",
        "plt.ylabel('Distancia (Ward)')  # Distancia entre clusters fusionados\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "boDPhF1svVRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3-wWhPb-QWV"
      },
      "outputs": [],
      "source": [
        "# Establecer formato: redondear a 1 decimal y desactivar notación científica\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "# Mostrar la matriz\n",
        "print(Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MYx3Qi_8Lin"
      },
      "source": [
        "¿Qué contiene la matriz de linkage? (**Matriz de enlaces**)\n",
        "\n",
        "Es una matriz de forma (n−1)×4, donde n es el número de puntos. Cada fila representa una fusión y contiene:\n",
        "\n",
        "- Índice del primer cluster/clase a fusionar\n",
        "- Índice del segundo cluster/clase a fusionar\n",
        "- Distancia entre los clusters fusionados (según el método elegido)\n",
        "- Número de elementos (puntos) en el nuevo cluster resultante\n",
        "\n",
        "\n",
        "**Explicación**\n",
        "\n",
        "* Nosotros tenemos 200 puntos (observaciones individuales), cuyos índices van del 0 al 199.\n",
        "\n",
        "* Cada vez que el algoritmo une dos puntos (o clusters), crea un nuevo cluster.\n",
        "\n",
        "* A cada nuevo cluster se le asigna un índice consecutivo, comenzando desde n.\n",
        "\n",
        "*Por ejemplo:*\n",
        "\n",
        "n = 200, entonces el siguiente cluster (la fusión del primer par) se le asigna el índice 200, luego 201, 202, ..., y así sucesivamente.\n",
        "\n",
        "Al final, en una agrupación jerárquica de n puntos, se necesitan exactamente n - 1 fusiones para unir todo en un único cluster. Por tanto, los índices llegarán hasta n + (n - 2) = 2n - 2.\n",
        "\n",
        "Concreatmente, **n = 200**, por lo tanto: el último índice asignado será **2n - 2 = 398**.\n",
        "\n",
        "El resultado de las últimas dos uniones, es decir, esa fusión es el cluster final que contiene los 200 elementos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# 7. CLUSTERING JERÁRQUICO (AGGLOMERATIVE) Final\n",
        "# ================================================\n",
        "\n",
        "n_clust = 3\n",
        "\n",
        "print(\"\\n---------------------------------------------------------------------\")\n",
        "print(f'CLUSTERING JERÁRQUICO (AGGLOMERATIVE) final con {n_clust} clusters')\n",
        "print(\"---------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "# Configuración del modelo\n",
        "cluster_jerarquico = AgglomerativeClustering(\n",
        "    n_clusters=n_clust,       # Número final de clusters a obtener\n",
        "    metric='euclidean',\n",
        "    linkage='ward'\n",
        ")\n",
        "\n",
        "# Aplicar el clustering a los datos escalados\n",
        "etiquetas_jer = cluster_jerarquico.fit_predict(X_scaled)  # Retorna un array con las etiquetas de cluster (0, 1, 2)\n",
        "print(etiquetas_jer)\n",
        "\n"
      ],
      "metadata": {
        "id": "i7YMWYpAvYWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# 8. VISUALIZACIÓN DE RESULTADOS FINALES\n",
        "# =============================================\n",
        "\n",
        "print(\"\\n---------------------------------------------\")\n",
        "print('VISUALIZACIÓN DE RESULTADOS FINALES')\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Graficar puntos coloreados por cluster\n",
        "plt.scatter(\n",
        "    X[:, 0],           # Peso en kg (eje X)\n",
        "    X[:, 1],           # Estatura en cm (eje Y)\n",
        "    c=etiquetas_jer,       # Color según etiqueta de cluster\n",
        "    cmap='viridis',    # Mapa de colores\n",
        "    alpha=0.7          # Transparencia para mejor visualización\n",
        ")\n",
        "plt.xlabel('Peso (kg)')\n",
        "plt.ylabel('Estatura (cm)')\n",
        "plt.colorbar(label='Cluster')  # Barra lateral que indica el mapeo de colores\n",
        "plt.grid(True, alpha=0.2)\n",
        "\n",
        "# Calcular y graficar centroides\n",
        "for cluster_id in np.unique(etiquetas_jer):\n",
        "    centroide = X[etiquetas_jer == cluster_id].mean(axis=0)  # Calcula la media de cada variable por cluster\n",
        "    plt.scatter(\n",
        "        centroide[0],          # Peso promedio del cluster\n",
        "        centroide[1],          # Estatura promedio del cluster\n",
        "        marker='*',            # Forma de estrella para el centroide\n",
        "        s=200,                 # Tamaño del marcador\n",
        "        c='red',               # Color rojo\n",
        "        edgecolor='black'      # Borde negro para mejor contraste\n",
        "    )\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "VBOmIGGxvcei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# 9. ANÁLISIS ESTADÍSTICO POR CLUSTER\n",
        "# =============================================\n",
        "\n",
        "print(\"\\n---------------------------------------------\")\n",
        "print('ANÁLISIS ESTADÍSTICO POR CLUSTER')\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "# Convertir a DataFrame para análisis\n",
        "df = df_inicial.copy()\n",
        "df['Cluster'] = etiquetas_jer  # Añadir columna de clusters\n",
        "\n",
        "\n",
        "# Estadísticas descriptivas por cluster\n",
        "tabla= df.groupby('Cluster').agg({\n",
        "    'Peso': ['mean', 'std'],      # Media y desviación estándar del peso\n",
        "    'Estatura': ['mean', 'std', 'count']   # Media, desviación estándar de la estatura y conteo\n",
        "})\n",
        "# Total de observaciones\n",
        "total = len(df)\n",
        "\n",
        "# Extraer el conteo desde la tabla (nivel de columna múltiple)\n",
        "conteo = tabla[('Estatura', 'count')]\n",
        "\n",
        "# Calcular proporción\n",
        "proporcion = ((conteo / total).round(3))*100  # Redondear a 4 decimales\n",
        "\n",
        "# Añadir como nueva columna\n",
        "tabla[('Estatura', 'proporcion')] = proporcion\n",
        "\n",
        "print(tabla)\n",
        "\n",
        "print('\\nCOMPARACIÓN CON LOS DATOS REALES')\n",
        "\n",
        "print(df_inicial.groupby('Grupo').agg({\n",
        "    'Peso': ['mean', 'std'],      # Media y desviación estándar del peso\n",
        "    'Estatura': ['mean', 'std', 'count']   # Media, desviación estándar de la estatura y conteo\n",
        "}))\n",
        "\n",
        "\n",
        "print('\\n MATRIZ DE CONFUSIÓN')\n",
        "#Matriz de confusión\n",
        "print(confusion_matrix(grupos, etiquetas_jer))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g8-1Yb1TvfYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos que no coinciden los números de los grupos, por lo que modificamos:\n",
        "mapeo = {\n",
        "    1: 2,  # El cluster 1 del modelo pasa a ser categoría 2\n",
        "    2: 1,  # El cluster 2 del modelo pasa a ser categoría 1\n",
        "    0: 0   # El cluster 0 se mantiene igual\n",
        "}\n",
        "\n",
        "# Función de mapeo\n",
        "def reemplazar_etiqueta(x):\n",
        "    return mapeo.get(x, x)  # Si la etiqueta no está en el diccionario, la deja igual\n",
        "\n",
        "# Aplicar el mapeo\n",
        "etiquetas_corregidas = np.vectorize(reemplazar_etiqueta)(etiquetas_jer)\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(grupos, etiquetas_corregidas))\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(grupos, etiquetas_corregidas))"
      ],
      "metadata": {
        "id": "7UQ4Vl2b-N6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yP-g2MC-QWW"
      },
      "source": [
        "### COMPARATIVA ENTRE CLUSTERING AGLOMERATIVO/NO AGLOMERATIVO\n",
        "\n",
        "| Característica                   | Aglomerativo                               | Divisivo                                               |\n",
        "| -------------------------------- | ------------------------------------------ | ------------------------------------------------------ |\n",
        "| **Enfoque**                      | Bottom-up (de abajo hacia arriba)          | Top-down (de arriba hacia abajo)                       |\n",
        "| **Proceso inicial**              | Cada punto es su propio clúster            | Todos los puntos están en un único clúster             |\n",
        "| **Paso típico**                  | Se fusionan los dos clústeres más cercanos | Se divide el clúster en subconjuntos                   |\n",
        "| **Técnica común**                | Enlace simple, completo, promedio, Ward    | Normalmente divisiones con k-means u otros heurísticos |\n",
        "| **Computacionalmente más común** | Más utilizado.                             | Menos común en la práctica                           |\n",
        "| **Complejidad computacional**    | O(n²) a O(n³) dependiendo del método       | Mayor si se buscan todas divisiones posibles           |\n",
        "| **Dendrograma**                  | Se construye de abajo hacia arriba         | Se construye de arriba hacia abajo                     |\n",
        "| **Ventaja principal**            | Fácil de implementar; bien soportado       | Puede capturar mejor la estructura global              |\n",
        "| **Desventaja principal**         | Puede ser sensible al orden de los datos   | Más costoso computacionalmente                         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8yC0SZokO64"
      },
      "source": [
        "### Clustering No Jerárquico\n",
        "\n",
        "Asigna puntos a clusters directamente, sin estructura jerárquica. Los clusters son planos y definitivos.\n",
        "\n",
        "**Algoritmos comunes:**\n",
        "\n",
        "* **K-means: minimiza la varianza intra-cluster.**\n",
        "\n",
        "* DBSCAN: basado en densidad y detección de outliers.\n",
        "\n",
        "**Ventaja:** Computacionalmente eficiente para grandes conjuntos de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzU9ebUEQUcz"
      },
      "source": [
        "**Cluster no jerárquico: K-Means**\n",
        "\n",
        "**Objetivo:** Dividir datos en k grupos (clusters) donde cada punto pertenece al grupo con la media (centroide) más cercana.\n",
        "\n",
        "**Pasos:**\n",
        "\n",
        "* Seleccionar k centroides aleatorios.\n",
        "\n",
        "* Asignar cada punto al centroide más cercano.\n",
        "\n",
        "* Recalcular los centroides.\n",
        "\n",
        "* Repetir hasta convergencia.\n",
        "\n",
        "Ejercicio:\n",
        "\n",
        "Teniendo en cuenta la base creada en el ejercicio anterior, realizar el agrupamiento de los pacientes utilizando el método **K-Means**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTxchKxv8v64"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------\n",
        "# 2. Aplicación de K-means\n",
        "# --------------------------------------------\n",
        "\n",
        "kmeans = KMeans(\n",
        "    n_clusters=3,       # Número de clusters (k=3 como en el ejemplo jerárquico)\n",
        "    init='k-means++',   # Método inteligente para inicializar centroides\n",
        "    n_init=10,          # Número de inicializaciones diferentes (evita resultados subóptimos)\n",
        "    max_iter=300,       # Máximo iteraciones por ejecución\n",
        "    random_state=42     # Semilla para reproducibilidad\n",
        ")\n",
        "# n_clusters=3\tNúmero de clusters (equivalente al usado en el jerárquico).\n",
        "# init='k-means++'\tInicializa centroides lejanos entre sí para evitar malas convergencias.\n",
        "# n_init=10\tEjecuta el algoritmo 10 veces con distintas inicializaciones y elige el mejor resultado.\n",
        "# max_iter=300\tNúmero máximo de iteraciones por ejecución.\n",
        "\n",
        "# Entrenamiento y predicción\n",
        "etiquetas_kmeans = kmeans.fit_predict(X_scaled)\n",
        "centroides_kmeans = scaler.inverse_transform(kmeans.cluster_centers_)  # Centroides en escala original\n",
        "\n",
        "print(centroides_kmeans)\n",
        "\n",
        "print(etiquetas_kmeans)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------\n",
        "# 3. Visualización de resultados\n",
        "# --------------------------------------------\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Gráfico de K-means\n",
        "plt.scatter(X[:, 0], X[:, 1], c=etiquetas_kmeans, cmap='viridis', alpha=0.7)\n",
        "plt.scatter(centroides_kmeans[:, 0], centroides_kmeans[:, 1],\n",
        "            marker='*', s=200, c='red', edgecolor='black', label='Centroides')\n",
        "plt.xlabel('Peso (kg)')\n",
        "plt.ylabel('Estatura (cm)')\n",
        "plt.title('K-means (k=3)')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3YLP1ohkwEqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# 4. ANÁLISIS ESTADÍSTICO POR CLUSTER\n",
        "# =============================================\n",
        "\n",
        "print('\\nANÁLISIS ESTADÍSTICO POR CLUSTER\\n')\n",
        "\n",
        "df_kmenas = df_inicial.copy()\n",
        "df_kmenas['Cluster_kmeans'] = etiquetas_kmeans\n",
        "\n",
        "# Estadísticas descriptivas por cluster\n",
        "tabla_kmeans= df_kmenas.groupby('Cluster_kmeans').agg({\n",
        "    'Peso': ['mean', 'std'],      # Media y desviación estándar del peso\n",
        "    'Estatura': ['mean', 'std', 'count']   # Media, desviación estándar de la estatura y conteo\n",
        "})\n",
        "# Total de observaciones\n",
        "total = len(df_kmenas)\n",
        "\n",
        "# Extraer el conteo desde la tabla (nivel de columna múltiple)\n",
        "conteo = tabla_kmeans[('Estatura', 'count')]\n",
        "\n",
        "# Calcular proporción\n",
        "proporcion = ((conteo / total).round(3))*100  # Redondear a 4 decimales\n",
        "\n",
        "# Añadir como nueva columna\n",
        "tabla_kmeans[('Estatura', 'proporcion')] = proporcion\n",
        "\n",
        "print(tabla_kmeans)\n",
        "\n",
        "print('\\nCOMPARACIÓN CON LOS DATOS REALES')\n",
        "\n",
        "print(df_inicial.groupby('Grupo').agg({\n",
        "    'Peso': ['mean', 'std'],      # Media y desviación estándar del peso\n",
        "    'Estatura': ['mean', 'std', 'count']   # Media, desviación estándar de la estatura y conteo\n",
        "}))\n",
        "\n",
        "print(\"\\nMATRIZ DE CONFUSIÓN\\n\")\n",
        "#Matriz de confusión\n",
        "print(confusion_matrix(grupos, etiquetas_kmeans))\n"
      ],
      "metadata": {
        "id": "edxR8s7ZwHae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos que no coinciden los números de los grupos, por lo que modificamos:\n",
        "mapeo = {\n",
        "    1: 2,  # El cluster 1 del modelo pasa a ser categoría 2\n",
        "    2: 1,  # El cluster 2 del modelo pasa a ser categoría 1\n",
        "    0: 0   # El cluster 0 se mantiene igual\n",
        "}\n",
        "\n",
        "# Función de mapeo\n",
        "def reemplazar_etiqueta(x):\n",
        "    return mapeo.get(x, x)  # Si la etiqueta no está en el diccionario, la deja igual\n",
        "\n",
        "# Aplicar el mapeo\n",
        "etiquetas_corregidas = np.vectorize(reemplazar_etiqueta)(etiquetas_kmeans)\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(grupos, etiquetas_corregidas))\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(grupos, etiquetas_corregidas))"
      ],
      "metadata": {
        "id": "DmtDIJhK_twr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSeACWPljIJO"
      },
      "source": [
        "### **Selección del Número Óptimo de Clusters**\n",
        "\n",
        "En el aprendizaje no supervisado, específicamente en problemas de clustering, uno de los desafíos fundamentales es determinar el número adecuado de grupos (*k*) que existen naturalmente en los datos. A diferencia del aprendizaje supervisado, donde contamos con etiquetas que guían el proceso, en clustering debemos descubrir esta estructura oculta mediante métodos analíticos.\n",
        "\n",
        "La elección incorrecta de *k* puede llevar a:\n",
        "- **Sub-segmentación** (*k* demasiado bajo): Grupos heterogéneos que pierden patrones importantes.\n",
        "- **Sobre-segmentación** (*k* demasiado alto): Grupos artificiales sin significado real, aumentando la complejidad del modelo.\n",
        "\n",
        "Para abordar este problema, existen métodos cuantitativos y visuales que nos ayudan a tomar esta decisión de manera objetiva. Dos de las técnicas más utilizadas son:\n",
        "\n",
        "1. **Método del Codo (Elbow Method)**: Un enfoque visual que analiza cómo mejora la cohesión de los clusters al aumentar *k*.\n",
        "2. **Silhouette Score**: Una medida cuantitativa que evalúa tanto la cohesión interna como la separación entre clusters.\n",
        "\n",
        "En el contexto de nuestro estudio con datos de peso y estatura, aplicaremos estos métodos para identificar cuántos grupos naturales de pacientes existen según sus características antropométricas, información valiosa para personalizar tratamientos o recomendaciones de salud. A continuación, exploraremos en detalle cada uno de estos métodos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAiQALJdjcOo"
      },
      "source": [
        "1. Método del Codo (Elbow Method) tiene como objetivo determinar el número óptimo de clusters (k) en algoritmos como K-means, evaluando la relación entre la varianza intra-cluster y el número de clusters.\n",
        "\n",
        "En primer lugar, la inercia (Suma de Cuadrados Intra-Cluster, WCSS) es la medida de cohesión que calcula la suma de las distancias al cuadrado entre cada punto y su centroide.\n",
        "Al incrementar k, la inercia disminuye (cada cluster es más compacto). El punto óptimo se encuentra donde la reducción de inercia comienza a ser marginal (forma de \"codo\" en la curva).\n",
        "\n",
        "2. Silhouette Score tiene como objetivo evaluar la calidad de los clusters midiendo cuán bien separados están y cuán cohesionados internamente.\n",
        "\n",
        "Para cada punto xi:\n",
        "\n",
        "* Distancia intra-cluster (ai): promedio de distancias entre xi y todos los puntos en su mismo cluster.\n",
        "\n",
        "* Distancia inter-cluster (bi): promedio de distancias entre xi y todos los puntos en el cluster más cercano (distinto al suyo).\n",
        "\n",
        "si∈[−1,1]:\n",
        "\n",
        "* Cercano a 1: El punto está bien asignado (lejos de clusters vecinos).\n",
        "\n",
        "+ Cercano a 0: El punto está en la frontera entre clusters.\n",
        "\n",
        "+ Cercano a -1: El punto está mal asignado (más cerca de otro cluster).\n",
        "\n",
        "El Silhouette Score global es el promedio de si para todos los puntos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzIUDYTWl4I6"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# Configuración de rango de k's a evaluar\n",
        "# =============================================\n",
        "\n",
        "k_range = range(2, 11)  # Creamos un rango de valores para k (número de clusters) desde 2 hasta 10\n",
        "\n",
        "\n",
        "# =============================================\n",
        "# Cálculo de métricas\n",
        "# =============================================\n",
        "\n",
        "inercias = []          # Lista para almacenar la inercia (WCSS) para cada k\n",
        "silhouette_scores = []  # Lista para almacenar los puntajes de silueta para cada k\n",
        "\n",
        "for k in k_range:\n",
        "    # Inicializamos KMeans con:\n",
        "    # - n_clusters=k: número de clusters a formar\n",
        "    # - random_state=42: semilla para reproducibilidad\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "\n",
        "    # Ajustamos el modelo y predecimos las etiquetas de cluster\n",
        "    etiquetas = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "    # Almacenamos la inercia (suma de distancias al cuadrado intra-cluster)\n",
        "    inercias.append(kmeans.inertia_)  # WCSS\n",
        "\n",
        "    # Calculamos y almacenamos el silhouette score para este k\n",
        "    # - X_scaled: datos estandarizados\n",
        "    # - etiquetas: asignaciones de cluster predichas\n",
        "    silhouette_scores.append(silhouette_score(X_scaled, etiquetas))\n",
        "\n",
        "# Creamos una figura de 15x5 pulgadas para los subplots\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# =============================================\n",
        "# Gráfico del Método del Codo (subplot izquierdo)\n",
        "# =============================================\n",
        "plt.subplot(1, 2, 1)  # 1 fila, 2 columnas, posición 1\n",
        "\n",
        "# Graficamos la curva del codo:\n",
        "# - k_range en eje X, inercias en eje Y\n",
        "# - 'bo-': puntos azules con línea continua\n",
        "# - markersize=8: tamaño de los puntos\n",
        "plt.plot(k_range, inercias, 'bo-', markersize=8)\n",
        "\n",
        "# Línea vertical roja punteada para marcar el codo sugerido\n",
        "plt.axvline(x=3, color='r', linestyle='--', alpha=0.7, label='Codo sugerido (k=3)')\n",
        "\n",
        "# Etiquetas y título\n",
        "plt.xlabel('Número de clusters (k)', fontsize=12)\n",
        "plt.ylabel('Inercia (WCSS)', fontsize=12)\n",
        "plt.title('Método del Codo', fontsize=14)\n",
        "\n",
        "# Configuramos ticks en el eje X para que coincidan con los valores de k probados\n",
        "plt.xticks(k_range)\n",
        "\n",
        "# Añadimos una cuadrícula semitransparente\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Mostramos la leyenda\n",
        "plt.legend()\n",
        "\n",
        "# =============================================\n",
        "# Gráfico del Silhouette Score (subplot derecho)\n",
        "# =============================================\n",
        "plt.subplot(1, 2, 2)  # 1 fila, 2 columnas, posición 2\n",
        "\n",
        "# Graficamos los silhouette scores:\n",
        "# - k_range en eje X, silhouette_scores en eje Y\n",
        "# - 'go-': puntos verdes con línea continua\n",
        "plt.plot(k_range, silhouette_scores, 'go-', markersize=8)\n",
        "\n",
        "# Línea vertical roja punteada para marcar el mejor score\n",
        "plt.axvline(x=3, color='r', linestyle='--', alpha=0.7, label='Mejor score (k=3)')\n",
        "\n",
        "# Etiquetas y título\n",
        "plt.xlabel('Número de clusters (k)', fontsize=12)\n",
        "plt.ylabel('Silhouette Score', fontsize=12)\n",
        "plt.title('Silhouette Score', fontsize=14)\n",
        "\n",
        "# Configuramos ticks en el eje X\n",
        "plt.xticks(k_range)\n",
        "\n",
        "# Añadimos cuadrícula\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Mostramos la leyenda\n",
        "plt.legend()\n",
        "\n",
        "# Ajustamos el layout para que no se solapen los elementos\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostramos la figura con ambos gráficos\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfjFgesZnst2"
      },
      "source": [
        "## Análisis de componentes principales (PCA)\n",
        "\n",
        "El Análisis de Componentes Principales (PCA) es una técnica fundamental de reducción de dimensionalidad en el aprendizaje automático y la estadística. Su objetivo es transformar un conjunto de variables correlacionadas en un nuevo conjunto de variables no correlacionadas llamadas componentes principales, ordenadas por la cantidad de varianza que explican en los datos originales.\n",
        "\n",
        "¿Por qué usar PCA?\n",
        "* Simplificar datos complejos: reduce el número de variables manteniendo la mayor información posible.\n",
        "\n",
        "* Visualizar datos multidimensionales: permite proyectar datos en 2D o 3D para su exploración visual.\n",
        "\n",
        "* Eliminar ruido y redundancia: Los componentes menos importantes suelen capturar ruido o información repetida.\n",
        "\n",
        "\n",
        "**Aplicación en el Ejemplo de Peso y Estatura**\n",
        "\n",
        "Para nuestros datos de pacientes:\n",
        "\n",
        "Variables originales: Peso (kg) y Estatura (cm).\n",
        "\n",
        "Problema: Aunque solo son 2 variables, PCA nos ayuda a:\n",
        "\n",
        "Entender cuál variable contribuye más a la variabilidad.\n",
        "\n",
        "Reducir a 1 componente si ambas están altamente correlacionadas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CJCw1PG-QWW"
      },
      "outputs": [],
      "source": [
        "# Datos estandarizados en dataframe:\n",
        "datos_scaled = pd.DataFrame(X_scaled,\n",
        "                     columns=[\"peso\", \"estatura\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3Ut5UUHmvtn"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# 1. Visualización de datos\n",
        "# =============================================\n",
        "# Código de visualización datos originales\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(df_inicial[\"Peso\"], df_inicial[\"Estatura\"],\n",
        "            marker=\"8\", s=400, color=\"purple\", alpha=0.5)\n",
        "plt.xlabel(\"Peso\", fontsize=15)\n",
        "plt.ylabel(\"Estatura\", fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matriz de covarianza**\n",
        "\n",
        "La covarianza es una medida de la variación conjunta de dos variables aleatorias, indicando si tienden a variar en la misma dirección (covarianza positiva) o en direcciones opuestas (covarianza negativa)."
      ],
      "metadata": {
        "id": "SiGlyEM4whCD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-bY2VQ5mvqQ"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# 2. Código de visualización datos escalados y la matriz de covarianzas\n",
        "# =============================================\n",
        "\n",
        "print(pd.DataFrame(X_scaled).cov())\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(datos_scaled[\"peso\"], datos_scaled[\"estatura\"],\n",
        "            marker=\"8\", s=400, color=\"purple\", alpha=0.5)\n",
        "plt.xlabel(\"Peso\", fontsize=15)\n",
        "plt.ylabel(\"Estatura\", fontsize=15)\n",
        "plt.text(-2.2, 1.5, \"Covarianza\\npositiva\", fontsize=18, color=\"red\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9mfOQYe-QWX"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# 3. Cálculo de eigenvalores y eigenvectores\n",
        "# =============================================\n",
        "\n",
        "valores, vectores = eig(datos_scaled.cov())\n",
        "\n",
        "vector_azul = vectores[:, 0]\n",
        "vector_rojo = vectores[:, 1]\n",
        "\n",
        "print(\"Eigenvector Rojo:\", vector_rojo, \"Eigenvalor:\", valores[1])\n",
        "print(\"Eigenvector Azul:\", vector_azul, \"Eigenvalor:\", valores[0])\n",
        "\n",
        "# Código de visualización\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.axes().set_aspect(\"equal\")\n",
        "\n",
        "# Graficando los datos\n",
        "plt.scatter(datos_scaled[\"peso\"], datos_scaled[\"estatura\"],\n",
        "            marker=\"8\", s=400, color=\"purple\", alpha=0.5)\n",
        "\n",
        "# Graficando los eigenvectores\n",
        "plt.quiver(0, 0,\n",
        "           vector_azul[0]/abs(vector_azul[0])*valores[0],\n",
        "           vector_azul[1]/abs(vector_azul[1])*valores[0],\n",
        "           color=\"blue\", angles=\"xy\", scale_units=\"xy\", scale=1, width=0.02)\n",
        "\n",
        "plt.quiver(0, 0,\n",
        "           vector_rojo[0]/abs(vector_rojo[0])*valores[1],\n",
        "           vector_rojo[1]/abs(vector_rojo[1])*valores[1],\n",
        "           color=\"red\", angles=\"xy\", scale_units=\"xy\", scale=1, width=0.02)\n",
        "\n",
        "plt.xlabel(\"peso\", fontsize=15)\n",
        "plt.ylabel(\"estatura\", fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh-MNSah-QWX"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# 4. Datos proyectados (nuevos ejes)\n",
        "# =============================================\n",
        "\n",
        "proyectados = pd.DataFrame(datos_scaled.values @ vectores.T,\n",
        "                          columns=[\"peso\", \"estatura\"])\n",
        "\n",
        "# Código de visualización\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.axes().set_aspect(\"equal\")\n",
        "\n",
        "# Datos rotados\n",
        "plt.scatter(proyectados[\"peso\"], proyectados[\"estatura\"],\n",
        "            marker=\"8\", s=400, color=\"purple\", alpha=0.5)\n",
        "\n",
        "# Proyección de los datos en 1 dimensión\n",
        "plt.scatter(proyectados[\"peso\"], [-3]*len(proyectados[\"estatura\"]),\n",
        "            s=250, color=\"red\", alpha=0.5)\n",
        "\n",
        "plt.scatter([-2]*len(proyectados[\"estatura\"]), proyectados[\"estatura\"],\n",
        "            s=250, color=\"blue\", alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaE39laP-QWX"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# 4. Obtención de componentes principales\n",
        "# =============================================\n",
        "\n",
        "pca = PCA()\n",
        "datos_scaled2 = pca.fit_transform(datos_scaled)\n",
        "\n",
        "# Código de visualización\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.axes().set_aspect(\"equal\")\n",
        "plt.scatter(datos_scaled2[:, 0], [2]*datos_scaled2[:, 0].size, s=250, color=\"red\", alpha=0.5)\n",
        "plt.scatter(datos_scaled2[:, 1], [1]*datos_scaled2[:, 1].size, s=250, color=\"blue\", alpha=0.5)\n",
        "plt.ylim((0.8, 2.2))\n",
        "plt.show()\n",
        "print(\"Varianza explicada por cada componente:\", pca.explained_variance_)\n",
        "print(\"Proporción de varianza explicada por cada componente:\", pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSRv6Ffa-QWY"
      },
      "source": [
        "### Interpretación de la primera componente principal (PC1)\n",
        "\n",
        "La primera componente principal (PC1) representa la **dirección de máxima varianza** en el conjunto de datos. En este caso, explica el **91 % de la varianza total**, lo que significa que **una sola dimensión puede resumir la mayor parte de la información contenida en las variables peso y estatura**.\n",
        "\n",
        "Matemáticamente, PC1 es una **combinación lineal** de las variables estandarizadas:\n",
        "\n",
        "$$\n",
        "\\text{PC1} = a_1 \\cdot \\text{peso (escalado)} + a_2 \\cdot \\text{estatura (escalada)}\n",
        "$$\n",
        "\n",
        "Por ejemplo, si:\n",
        "\n",
        "$$\n",
        "a_1 \\approx 0.71 \\quad \\text{y} \\quad a_2 \\approx 0.71\n",
        "$$\n",
        "\n",
        "entonces:\n",
        "\n",
        "$$\n",
        "\\text{PC1} = 0.71 \\cdot \\text{peso (escalado)} + 0.71 \\cdot \\text{estatura (escalada)}\n",
        "$$\n",
        "\n",
        "Esto significa que **PC1 captura una medida de \"tamaño corporal general\"**, ya que da pesos positivos similares tanto al peso como a la estatura.\n",
        "\n",
        "En términos prácticos:\n",
        "\n",
        "- Valores altos de PC1 indican personas **más grandes** (más altas y pesadas).\n",
        "- Valores bajos de PC1 indican personas **más pequeñas** (más bajas y livianas).\n",
        "\n",
        "Esto justifica que, al proyectar los datos sobre esta componente, obtengamos una representación más compacta sin perder gran parte de la información original.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJPK9LUq-QWY"
      },
      "source": [
        "### Interpretación de la segunda componente principal (PC2)\n",
        "\n",
        "La segunda componente principal (PC2) es ortogonal a la primera y representa la **dirección de variación residual**, es decir, lo que no fue explicado por la PC1. Esta componente explica apenas el 9 % de la variabilidad total del conjunto de datos.\n",
        "\n",
        "Matemáticamente, PC2 también es una combinación lineal de las variables estandarizadas:\n",
        "\n",
        "$$\n",
        "\\text{PC2} = b_1 \\cdot \\text{peso (escalado)} + b_2 \\cdot \\text{estatura (escalada)}\n",
        "$$\n",
        "\n",
        "Por ejemplo, si:\n",
        "\n",
        "$$\n",
        "b_1 \\approx -0.71 \\quad \\text{y} \\quad b_2 \\approx 0.71\n",
        "$$\n",
        "\n",
        "entonces:\n",
        "\n",
        "$$\n",
        "\\text{PC2} = -0.71 \\cdot \\text{peso (escalado)} + 0.71 \\cdot \\text{estatura (escalada)}\n",
        "$$\n",
        "\n",
        "Este tipo de combinación sugiere que PC2 refleja una **diferencia o contraposición** entre estatura y peso. En términos prácticos:\n",
        "\n",
        "- Un valor alto de PC2 indica que la persona es **alta y liviana** en comparación con el promedio.\n",
        "- Un valor bajo de PC2 indica que la persona es **baja y pesada**.\n",
        "\n",
        "Dado que esta componente explica una proporción pequeña de la varianza (9 %), su utilidad principal es para observar **variaciones atípicas o específicas** en la relación entre peso y estatura que no se explican por el tamaño general del cuerpo (PC1).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad"
      ],
      "metadata": {
        "id": "ls8N8uNeJ-SM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teniendo en cuenta el dataset de diabetes trabajado anteriormente, realiza el agrupamiento por medio de clustering (tanto aglomerativo como k-means)"
      ],
      "metadata": {
        "id": "vQMr2goDI6zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar la base de datos\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "columns = ['Embarazos', 'Glucosa', 'Presion', 'Piel', 'Insulina', 'IMC', 'Pedigri', 'Edad', 'Resultado']\n",
        "data = pd.read_csv(url, header=None, names=columns)\n"
      ],
      "metadata": {
        "id": "B8M_C4Gp_7pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Clustering"
      ],
      "metadata": {
        "id": "zKkpLrgPJv00"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGQdkVfuJuTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. PCA"
      ],
      "metadata": {
        "id": "zdW2bl1VLz8c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TeQWAoBcL6Dl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}